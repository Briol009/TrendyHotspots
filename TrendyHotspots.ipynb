{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b358fbc9-2643-4e37-94e9-9124367f9ed6",
   "metadata": {},
   "source": [
    "# MVP 3 Parallel Computing\n",
    "\n",
    "Laure Briol aided by Generative AI (ChatGPT)\n",
    "\n",
    "This MVP is designed to generate a heatmap of the Twin Cities area, showing 'trendy' locations. Trendy locations tend to have more people visitng the stores, as well as more people reviewing stores and shops in the area. So, a place with more recent reviews will be more 'trendy'. The Google's Places API was used to find locations of coffee shops and gift shops within the Twin Cities area. From there a heatmap was created, based on the average time of the 5 most recent reviews. This program used Dask to split up the API calls across the Twin Cities area, so we could 'talk' with Google many times at once, in order to ensure the project map could be created within a timely manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246411a-e58d-47a4-95ce-74c0ae6b802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries for handling requests, data manipulation, mapping, and parallel processing\n",
    "import requests\n",
    "import pandas as pd\n",
    "import folium\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "from folium.plugins import HeatMap\n",
    "import math\n",
    "import json\n",
    "#dask package for use in running multiple tasks at once\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from shapely.geometry import Polygon\n",
    "#grid pattern creation\n",
    "from shapely.ops import unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "554351c0-5db6-47dd-93a3-3ed1c0034280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter grid cell size in meters (e.g., 1000):  3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate total API requests to be made: 13440\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to proceed? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique places found: 1787\n",
      "Map has been saved to 'trendiness_heatmap.html'\n"
     ]
    }
   ],
   "source": [
    "#configuration and constants\n",
    "\n",
    "#replace with your actual google api key\n",
    "#note this was very expensive to run ($70 dollars for 3 Km) -- please dont run\n",
    "#run if you create another Google Account and have free trial credits\n",
    "API_KEY = '' \n",
    "\n",
    "#keywords to search for in the Google Places API\n",
    "KEYWORDS = ['coffee shop', 'gift shop']\n",
    "\n",
    "#in the final map, we show the top 3 locations instead of all 20 to make sure the map isn't too full\n",
    "#within each cell of the final heatmap, only show up to 3 point locations instead of 20.\n",
    "TOP_N = 3 \n",
    "\n",
    "#bounding box coordinates for the Twin Cities area, defining the geographic area of interest\n",
    "MIN_LAT, MAX_LAT = 44.75, 45.15\n",
    "MIN_LNG, MAX_LNG = -93.65, -92.9\n",
    "\n",
    "#number of parallel tasks to run, this is how many times at once we are calling Google's API to talk to\n",
    "#we call first --> 8 grid cells to search at one time to find different places\n",
    "#then we call second --> 8 places at one to find reviews of each place\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "\n",
    "#functions section\n",
    "#these are all the pieces of the code that we define now, then call later to run the program\n",
    "\n",
    "#function to convert meters to degrees, based on latitude\n",
    "#when creating the 'grid', we ask what size in meters because that is an easy number to interpret, but our bounding box is in lat/lng coordinates\n",
    "#so, we convert the meters to lat/lng coordinates so we can properly create a grid inside the bounding box in the correct unit size\n",
    "def meters_to_degrees(meters, latitude):\n",
    "    #every degree of latitude is the same number of meters\n",
    "    meters_per_degree_lat = 111_320\n",
    "    #each degree of longitude is dependent on where on the earth we are. further north there is less distance per degree of longitude\n",
    "    meters_per_degree_lng = 111_320 * math.cos(math.radians(latitude))\n",
    "    #given our grid size (like 3km), this tells us how many degrees of latitude 3km is.\n",
    "    delta_lat = meters / meters_per_degree_lat\n",
    "    #given our grid size (like 3km), this tells us how many degrees of longitude 3km is.\n",
    "    delta_lng = meters / meters_per_degree_lng\n",
    "    #returns this data back so we can create a proper grid cell that appears nicely on the map\n",
    "    return delta_lat, delta_lng\n",
    "\n",
    "#this function is used to split our list of API calls (like all the locations we want to search or all the places we want to get reviews from)\n",
    "#into n or 8 'chunks' that we want the computer to process\n",
    "def split_list(lst, n):\n",
    "    #calculate the size of each chunk by diving how mnay API calls we want to make by n\n",
    "    chunk_size = len(lst) // n\n",
    "    #calculate remainder\n",
    "    remainder = len(lst) % n\n",
    "    #create a list that has each chunk of data\n",
    "    #Each chunk is a list of API calls. The dask program will go through all 8 lists at the same time to get the data we want\n",
    "    chunks = []\n",
    "    #start at the start of the list, this is to keep track of how many pieces of data are in the chunk.\n",
    "    start = 0\n",
    "    #we are looping to give each 'chunk' a bit of data to process\n",
    "    for i in range(n):\n",
    "        #this calculates how many pieces of data to give the chunk to process\n",
    "        end = start + chunk_size + (1 if i < remainder else 0)\n",
    "        #take our list of API calls, and assign the chunk of data that section of the list\n",
    "        chunk = lst[start:end]\n",
    "        #add this chunk into the list of chunks for storage\n",
    "        chunks.append(chunk)\n",
    "        #we will look at the next bit of the list to split up\n",
    "        start = end\n",
    "    #at the end, we return the list of chunks.\n",
    "    return chunks\n",
    "\n",
    "#this funciton is designed to call Google's API and search for all the 'places' that are inside our grid cell size\n",
    "def get_places(keyword, location, radius, api_key):\n",
    "    #this is the google API website URL that we want to talk to, gives us all the 'places' within a nearby area of our search\n",
    "    url = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json'\n",
    "    #this is the parameters telling the Google API what we are searching for (keyword), where we want to search (location), how far to search (radius), and API key for authentication\n",
    "    params = {\n",
    "        'keyword': keyword,\n",
    "        'location': location,\n",
    "        'radius': radius,\n",
    "        'key': api_key\n",
    "    }\n",
    "    #use the requests package to 'call' the Google API and get data\n",
    "    response = requests.get(url, params=params)\n",
    "    #convert the data that google gives us into JSON format for easier processing\n",
    "    res_json = response.json()\n",
    "    #within the data from google, we specifically want the results. They give us other not useful metadata but we only care about the places that it gives us\n",
    "    results = res_json.get('results', [])\n",
    "    #return the list of places to the code below\n",
    "    return results\n",
    "\n",
    "#this funciton is designed to call Google's API and search for the reviews of each 'place' that we found from the get_places funciton above\n",
    "def get_place_details(place_id, api_key):\n",
    "    #this is the google API website URL that we want to talk to, gives us details of a given 'place'\n",
    "    url = 'https://maps.googleapis.com/maps/api/place/details/json'\n",
    "    #this is the parameters telling the Google API what we place are searching for (place_id), what we are interested in learning about that place (fields), we want the reviews sorted giving the newest reviews, and API key for authentication\n",
    "    params = {\n",
    "        'place_id': place_id,\n",
    "        'fields': 'name,geometry,review,formatted_address,website',\n",
    "        'reviews_sort': 'newest',\n",
    "        'key': api_key\n",
    "    }\n",
    "    #use the requests package to 'call' the Google API and get data\n",
    "    response = requests.get(url, params=params)\n",
    "    #return the details about the place, we want specifically the results of the API call\n",
    "    return response.json().get('result', {})\n",
    "\n",
    "#this is the function that performs all the repetitive API calls to google to search each individual grid cell.\n",
    "#it takes in a 'chunk' of API calls we want to make, and then goes through that list one at a time calling the Google API for each grid cell location\n",
    "def process_chunk(chunk, api_key):\n",
    "    #create a list to hold all the individual places that we find\n",
    "    places_list = []\n",
    "    #create a loop that looks at each individual cell location and then searches based on the 2 keywords and grid cell size\n",
    "    for keyword, location, radius in chunk:\n",
    "        #at each grid cell location, call the Google API to search for all the business locations inside that cell\n",
    "        places = get_places(keyword, location, radius, api_key)\n",
    "        #once we get a list of all the locations, we add in extra info that this place was a 'coffee shop' or 'gift shop' based on the search we did\n",
    "        for place in places:\n",
    "            place['keyword'] = keyword\n",
    "        #we add onto the list holding all places the new places we found from the Google API call\n",
    "        places_list.extend(places)\n",
    "        #we slow down the program, pausing for 0.1 second so that Google doesn't get mad for calling too many times at once.\n",
    "        time.sleep(0.1)\n",
    "    #once we finish searching all the grid cells in this 'chunk' we return all the places that it was able to find. \n",
    "    return places_list\n",
    "\n",
    "#now that we have a list of locations, this is calling Google's API again to get the reviews at each location\n",
    "#this takes in a 'chunk' list of business locations, and it then gets all the reviews for that location\n",
    "def process_chunk_response(place_chunk, api_key):\n",
    "    #initialize a list to hold detailed data about each place\n",
    "    data = []\n",
    "    #loop over each place in the chunk to retrieve detailed information\n",
    "    for place in place_chunk:\n",
    "        #get the place id, a unique identifier for each place\n",
    "        place_id = place['place_id']\n",
    "        #retrieve place details using the place id and api key\n",
    "        details = get_place_details(place_id, api_key)\n",
    "        #get the list of reviews from the place details\n",
    "        reviews = details.get('reviews', [])\n",
    "        #check if there are any reviews available for the place\n",
    "        if reviews:\n",
    "            #get up to the first 5 reviews, which are already sorted by newest\n",
    "            recent_reviews = reviews[:5]\n",
    "            #calculate the average timestamp of the recent reviews to determine trendiness\n",
    "            avg_time = sum([review['time'] for review in recent_reviews]) / len(recent_reviews)\n",
    "            #convert the average timestamp to a datetime object for readability\n",
    "            avg_datetime = datetime.fromtimestamp(avg_time)\n",
    "            #append the collected data for this place to the data list\n",
    "            data.append({\n",
    "                'name': details.get('name', ''),\n",
    "                'lat': details['geometry']['location']['lat'],\n",
    "                'lng': details['geometry']['location']['lng'],\n",
    "                'avg_review_time': avg_datetime,\n",
    "                'address': details.get('formatted_address', ''),\n",
    "                'website': details.get('website', ''),\n",
    "                'keyword': place.get('keyword', '')\n",
    "            })\n",
    "        #delay to respect api rate limits, ensuring compliance with api usage policies\n",
    "        time.sleep(0.1)\n",
    "    #return the list of detailed data collected from this chunk\n",
    "    return data\n",
    "\n",
    "#this function is used to create the final heatmap of the data based on the average time of the 5 most recent reviews\n",
    "#the more recent the reviews, the 'hotter' a place is on the map.\n",
    "def prepare_heatmap_data(data):\n",
    "    #get the current time as a timestamp to calculate time differences\n",
    "    current_time = time.time()\n",
    "    #initialize a list to hold heatmap data points\n",
    "    heat_data = []\n",
    "    #loop over each item in the data to calculate heatmap intensities\n",
    "    for item in data:\n",
    "        #calculate the time difference between now and the average review time\n",
    "        time_diff = current_time - item['avg_review_time'].timestamp()\n",
    "        #check if the time difference is positive to ensure valid calculations\n",
    "        if time_diff > 0:\n",
    "            #avoid division by zero by checking the time difference\n",
    "            intensity = 1 / time_diff if time_diff != 0 else 0\n",
    "            #append the heatmap data point with latitude, longitude, and calculated intensity\n",
    "            heat_data.append([item['lat'], item['lng'], intensity])\n",
    "    #return the prepared heatmap data for visualization\n",
    "    return heat_data\n",
    "\n",
    "#this function is desinged to create the final map that is saved to 'trendiness_heatmap.html'\n",
    "#it combines the information that we collected (place points), details about each point (click on a point it gives details), the grid shape overlay, and the heatmap\n",
    "#this is a long function, but it creates a folium map featuring all those items listed. Most of the code is defining the colors, shapes, legend features for the folium map\n",
    "def create_map(heat_data, data, grid_polygons, filename='trendiness_heatmap.html'):\n",
    "    #center the map around the twin cities by calculating the midpoint of the bounding box\n",
    "    center_lat = (MIN_LAT + MAX_LAT) / 2\n",
    "    center_lng = (MIN_LNG + MAX_LNG) / 2\n",
    "    #create a folium map object centered at the calculated midpoint with an initial zoom level\n",
    "    m = folium.Map(location=[center_lat, center_lng], zoom_start=11)\n",
    "    #add heatmap layer to the map using the prepared heatmap data and define visual properties\n",
    "    HeatMap(heat_data, radius=15, name='Heatmap').add_to(m)\n",
    "    \n",
    "    #create a feature group for the grid, allowing grid cells to be managed as a single layer\n",
    "    grid_group = folium.FeatureGroup(name='Grid', show=True)\n",
    "    #add grid cells to the map by iterating over each polygon defining a grid cell\n",
    "    for polygon in grid_polygons:\n",
    "        #convert polygon coordinates to the format required by folium (latitude, longitude)\n",
    "        coords = [(lat, lng) for lng, lat in polygon.exterior.coords]\n",
    "        #create a polygon on the map with specified visual properties and add it to the grid group\n",
    "        folium.Polygon(\n",
    "            locations=coords,\n",
    "            color='grey',\n",
    "            weight=1,\n",
    "            fill=False,\n",
    "            opacity=0.9\n",
    "        ).add_to(grid_group)\n",
    "    #add the grid group to the map, making the grid visible as a separate layer\n",
    "    m.add_child(grid_group)\n",
    "    \n",
    "    #create a feature group for each keyword to manage markers by category\n",
    "    keyword_groups = {}\n",
    "    #define a list of colors to differentiate keywords visually on the map\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'darkred', 'lightred', 'beige', 'darkblue', 'darkgreen']\n",
    "    \n",
    "    #assign a unique color to each keyword for visual distinction in the markers\n",
    "    keyword_colors = {keyword: colors[i % len(colors)] for i, keyword in enumerate(KEYWORDS)}\n",
    "    \n",
    "    #allow toggling of the marker layers for each keyword, enhancing interactivity\n",
    "    for keyword in KEYWORDS:\n",
    "        #create a feature group for the current keyword\n",
    "        fg = folium.FeatureGroup(name=keyword, show=True)\n",
    "        #store the feature group in the keyword_groups dictionary for easy access\n",
    "        keyword_groups[keyword] = fg\n",
    "        #add the feature group to the map, enabling it to be displayed as a separate layer\n",
    "        m.add_child(fg)\n",
    "    \n",
    "    #group data by grid cells and keywords to organize markers efficiently\n",
    "    from collections import defaultdict\n",
    "    #initialize a nested defaultdict to categorize places by grid cell and keyword\n",
    "    grid_data = defaultdict(lambda: defaultdict(list))\n",
    "    #define grid size based on previously calculated degree deltas\n",
    "    grid_size = (delta_lat, delta_lng)\n",
    "    \n",
    "    #loop over each data item to assign it to the appropriate grid cell and keyword category\n",
    "    for item in data:\n",
    "        #calculate the latitude index of the grid cell for the current item\n",
    "        lat_idx = int((item['lat'] - MIN_LAT) / delta_lat)\n",
    "        #calculate the longitude index of the grid cell for the current item\n",
    "        lng_idx = int((item['lng'] - MIN_LNG) / delta_lng)\n",
    "        #define the grid cell as a tuple of latitude and longitude indices\n",
    "        grid_cell = (lat_idx, lng_idx)\n",
    "        #retrieve the keyword associated with the current item\n",
    "        keyword = item['keyword']\n",
    "        #append the current item to the appropriate grid cell and keyword category\n",
    "        grid_data[grid_cell][keyword].append(item)\n",
    "    \n",
    "    #for each grid cell and keyword, select top N places based on recency to display as markers\n",
    "    for grid_cell in grid_data:\n",
    "        for keyword in grid_data[grid_cell]:\n",
    "            #sort places by avg_review_time in descending order to prioritize recent reviews\n",
    "            places = sorted(grid_data[grid_cell][keyword], key=lambda x: x['avg_review_time'], reverse=True)\n",
    "            #select the top N places from the sorted list\n",
    "            top_places = places[:TOP_N]\n",
    "            #add markers for each of the top places to the corresponding keyword feature group\n",
    "            for place in top_places:\n",
    "                #create a popup with additional details about the place for user interaction\n",
    "                popup_html = f\"<b>{place['name']}</b><br>\"\n",
    "                popup_html += f\"Keyword: {place['keyword']}<br>\"\n",
    "                popup_html += f\"Address: {place['address']}<br>\"\n",
    "                if place['website']:\n",
    "                    popup_html += f\"<a href='{place['website']}' target='_blank'>Website</a>\"\n",
    "                #create a folium Popup object with the constructed HTML content\n",
    "                popup = folium.Popup(popup_html, max_width=300)\n",
    "                #add a marker to the map at the place's location with the popup and a colored icon\n",
    "                folium.Marker(\n",
    "                    location=[place['lat'], place['lng']],\n",
    "                    popup=popup,\n",
    "                    icon=folium.Icon(color=keyword_colors[keyword], icon='info-sign'),\n",
    "                ).add_to(keyword_groups[keyword])\n",
    "    \n",
    "    #add layer control to toggle marker and grid layers, enhancing map interactivity\n",
    "    folium.LayerControl().add_to(m)\n",
    "    #save the constructed map to an HTML file for viewing in a web browser\n",
    "    m.save(filename)\n",
    "    #print a confirmation message indicating where the map has been saved\n",
    "    print(f\"Map has been saved to '{filename}'\")\n",
    "\n",
    "\n",
    "\n",
    "#main file running\n",
    "#this section is where we call all the functions that were writeen above and perform the process of actually creating the map\n",
    "\n",
    "#start by getting the size of our grid cells by the user\n",
    "grid_cell_size = float(input(\"Enter grid cell size in meters (e.g., 1000): \"))\n",
    "\n",
    "#convert grid cell size from meters to degrees using the helper function, this is used so we can calculate the number of grid cells on the map\n",
    "avg_latitude = (MIN_LAT + MAX_LAT) / 2\n",
    "delta_lat, delta_lng = meters_to_degrees(grid_cell_size, avg_latitude)\n",
    "\n",
    "#now, we are calulating the number of grid cells, so we know how many API calls we may call\n",
    "#calculate the number of steps for latitude based on the bounding box and grid size\n",
    "lat_steps = int(math.ceil((MAX_LAT - MIN_LAT) / delta_lat))\n",
    "#calculate the number of steps for longitude based on the bounding box and grid size\n",
    "lng_steps = int(math.ceil((MAX_LNG - MIN_LNG) / delta_lng))\n",
    "\n",
    "#initialize lists to hold latitude and longitude points, which define the grid cell locations\n",
    "lat_points = []\n",
    "lng_points = []\n",
    "\n",
    "#create a list of latitude points\n",
    "for i in range(lat_steps + 1):\n",
    "    #calculate the latitude point by adding the delta to the minimum latitude\n",
    "    lat_point = MIN_LAT + i * delta_lat\n",
    "    #append the calculated latitude point to the list\n",
    "    lat_points.append(lat_point)\n",
    "\n",
    "#create a list of longitude points\n",
    "for i in range(lng_steps + 1):\n",
    "    #calculate the longitude point by adding the delta to the minimum longitude\n",
    "    lng_point = MIN_LNG + i * delta_lng\n",
    "    #append the calculated longitude point to the list\n",
    "    lng_points.append(lng_point)\n",
    "\n",
    "#generate grid coordinates by pairing each latitude point with each longitude point\n",
    "#this is giving the point at the center of each grid cell so we can call in the google API later\n",
    "grid_points = []\n",
    "#loop over each latitude point\n",
    "for lat in lat_points:\n",
    "    #loop over each longitude point for the current latitude\n",
    "    for lng in lng_points:\n",
    "        #append the coordinate pair as a tuple to the grid points list\n",
    "        grid_points.append((lat, lng))\n",
    "\n",
    "#this creates a grid-shaped polygon, this is used for creating the grid overlay on the map later\n",
    "grid_polygons = []\n",
    "#loop over the number of latitude steps to create horizontal grid lines\n",
    "for i in range(lat_steps):\n",
    "    #loop over the number of longitude steps to create vertical grid lines\n",
    "    for j in range(lng_steps):\n",
    "        #get the four corners of the current grid cell based on latitude and longitude indices\n",
    "        lat1 = lat_points[i]\n",
    "        lat2 = lat_points[i + 1]\n",
    "        lng1 = lng_points[j]\n",
    "        lng2 = lng_points[j + 1]\n",
    "        #create a polygon for the current grid cell using the four corner coordinates\n",
    "        polygon = Polygon([\n",
    "            (lng1, lat1),\n",
    "            (lng2, lat1),\n",
    "            (lng2, lat2),\n",
    "            (lng1, lat2),\n",
    "            (lng1, lat1)\n",
    "        ])\n",
    "        #append the created polygon to the grid_polygons list\n",
    "        grid_polygons.append(polygon)\n",
    "\n",
    "\n",
    "#now, we are preparing to make API calls using the google API, but first we want to ensure it won't cost too much money\n",
    "#set the search radius to half the grid cell size\n",
    "radius = grid_cell_size / 2\n",
    "\n",
    "#this is the task inputs (aka what keyword we want to search for, and the locations of all the grid cells)\n",
    "task_inputs = []\n",
    "#loop over each keyword to create search tasks for each category\n",
    "for keyword in KEYWORDS:\n",
    "    #loop over each grid point to define the search location\n",
    "    for lat, lng in grid_points:\n",
    "        #the Google API want the locations in latitude, then longitude format\n",
    "        location = f\"{lat},{lng}\"\n",
    "        #this adds onto the list\n",
    "        task_inputs.append((keyword, location, radius))\n",
    "\n",
    "#so now, we have a list of 'tasks' aka all the locations we need to search and each search term (coffee shops & gift shops)\n",
    "\n",
    "#estimate total api requests to warn the user about how many total API calls it may take\n",
    "#it will for sure take 1 API call per grid cell * the number of keywords (2) * worst case scenario 20 API calls per grid cell location\n",
    "#the worst case is that there are 20 businesses within the grid cell, meaning we need to get reviews for all 20 locations.\n",
    "total_requests = len(task_inputs) * 20\n",
    "#print the estimated total number of api requests to be made\n",
    "print(f\"Approximate total API requests to be made: {total_requests}\")\n",
    "\n",
    "#prompt the user for confirmation before proceeding, ensuring they are aware of the potential number of requests\n",
    "confirm = input(\"Do you want to proceed? (yes/no): \").strip().lower()\n",
    "#check if the user confirmed to continue\n",
    "if confirm != 'yes':\n",
    "    #if the user did not confirm, print a cancellation message and exit the script\n",
    "    print(\"Operation cancelled by the user.\")\n",
    "    exit()\n",
    "\n",
    "#split tasks into chunks to be able to use dask to run multiple things at once\n",
    "\n",
    "#split the task inputs list into smaller chunks based on the number of workers (8)\n",
    "chunks = split_list(task_inputs, NUM_WORKERS)\n",
    "\n",
    "\n",
    "#start dask to manage the parallel computer workload using multiple workers\n",
    "\n",
    "#create a local dask cluster with the specified number of workers and single thread per worker\n",
    "cluster = LocalCluster(n_workers=NUM_WORKERS, threads_per_worker=1)\n",
    "#create a dask client connected to the local cluster, enabling task distribution and management\n",
    "client = Client(cluster)\n",
    "\n",
    "#initialize a list to hold chunk tasks, which are units of work to be run at the same time\n",
    "chunk_tasks = []\n",
    "#loop over each chunk of tasks\n",
    "for chunk in chunks:\n",
    "    #create a delayed task for processing the current chunk using the process_chunk function\n",
    "    task = delayed(process_chunk)(chunk, API_KEY)\n",
    "    #append the created delayed task to the chunk_tasks list\n",
    "    chunk_tasks.append(task)\n",
    "\n",
    "#tell dask to start running all the API calls to search for place locations and save the list of places into a list of lists\n",
    "all_places_lists = compute(*chunk_tasks)\n",
    "\n",
    "#now we combine all the results from the multiple API calls into one big list\n",
    "all_places = []\n",
    "#loop over the results from each chunk\n",
    "for places in all_places_lists:\n",
    "    #extend the all_places list with the places retrieved from the current chunk\n",
    "    all_places.extend(places)\n",
    "\n",
    "#save all retrieved places to a JSON file for record-keeping and potential future use\n",
    "with open('all_places.json', 'w') as f:\n",
    "    json.dump(all_places, f)\n",
    "\n",
    "\n",
    "#remove any duplicate places incase the Google API gave us duplicate locations\n",
    "unique_places = {}\n",
    "#loop over each place in the all_places list\n",
    "for place in all_places:\n",
    "    #get the place id, which uniquely identifies each place\n",
    "    place_id = place['place_id']\n",
    "    #add the place to the unique_places dictionary using place_id as the key\n",
    "    unique_places[place_id] = place\n",
    "\n",
    "#this removes all the duplicate places\n",
    "all_places = list(unique_places.values())\n",
    "#print the total number of unique places found\n",
    "print(f\"Total unique places found: {len(all_places)}\")\n",
    "\n",
    "\n",
    "#now, we run dask again to get reviews of all the individual businesses\n",
    "place_chunks = split_list(all_places, NUM_WORKERS)\n",
    "\n",
    "#initialize a list to hold place review chunks\n",
    "detail_chunk_tasks = []\n",
    "#loop over each place chunk\n",
    "for chunk in place_chunks:\n",
    "    #create a delayed task for processing the current place chunk using the process_chunk_response function\n",
    "    task = delayed(process_chunk_response)(chunk, API_KEY)\n",
    "    #append the created delayed task to the detail_chunk_tasks list\n",
    "    detail_chunk_tasks.append(task)\n",
    "\n",
    "#tell dask to start making all the API calls to get review information\n",
    "results = compute(*detail_chunk_tasks)\n",
    "\n",
    "#now we combine data from all chunks into a single list containing detailed information about each place\n",
    "data = []\n",
    "#loop over the results from each detail chunk\n",
    "for res in results:\n",
    "    #extend the data list with the detailed information retrieved from the current chunk\n",
    "    data.extend(res)\n",
    "\n",
    "#prepare heatmap data by calculating intensities based on the recency of reviews\n",
    "heat_data = prepare_heatmap_data(data)\n",
    "\n",
    "#create and save the map with heatmap, markers, and grid to visualize the collected and processed data\n",
    "create_map(heat_data, data, grid_polygons)\n",
    "\n",
    "#convert 'avg_review_time' to string format because that lets us save the data easier\n",
    "for item in data:\n",
    "    item['avg_review_time'] = item['avg_review_time'].isoformat()\n",
    "\n",
    "#save detailed data to a JSON file for future reference\n",
    "with open('detailed_places_data.json', 'w') as f:\n",
    "    json.dump(data, f)\n",
    "\n",
    "#close the dask client so it doesn't run forever\n",
    "client.close()\n",
    "#close the dask cluster so it doesn't take up computer power forever\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
